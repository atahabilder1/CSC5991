{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 (Slides 2-5): Supervised Learning Basics\n",
    "## CSC5991 - Introduction to LLMs\n",
    "\n",
    "---\n",
    "\n",
    "**Welcome!** This notebook is designed for students who have **never studied machine learning before**. We will walk through every concept step by step, with plain-English explanations, real code examples, and visualizations.\n",
    "\n",
    "### What You Will Learn in This Notebook\n",
    "\n",
    "| Slide | Topic | What You Will Understand |\n",
    "|-------|-------|-------------------------|\n",
    "| Slide 2 | What is Supervised Learning? | The core definition, labeled data, inputs/outputs, and deep learning basics |\n",
    "| Slide 3 | Examples of Supervised Learning | 10+ real-world applications across many industries |\n",
    "| Slide 4 | Supervised Learning Workflow | The step-by-step pipeline from data to predictions |\n",
    "| Slide 5 | Types of Supervised Learning | Regression vs. Classification with hands-on code |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic Python knowledge (variables, loops, functions)\n",
    "- No machine learning experience required!\n",
    "\n",
    "### Libraries We Will Use\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|--------|\n",
    "| `numpy` | Numerical computations (arrays, math) |\n",
    "| `matplotlib` | Creating charts and visualizations |\n",
    "| `scikit-learn` | Machine learning algorithms and tools |\n",
    "| `pandas` | Data manipulation (tables) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 0: Import all the libraries we need for this notebook\n",
    "# =============================================================================\n",
    "\n",
    "# numpy: the fundamental library for numerical computing in Python.\n",
    "# We use it to create arrays (lists of numbers) and do math on them.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib.pyplot: the most popular plotting library in Python.\n",
    "# We use it to create charts, graphs, and visualizations.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pandas: a library for working with tabular data (like spreadsheets).\n",
    "# We use it to organize data into neat tables.\n",
    "import pandas as pd\n",
    "\n",
    "# scikit-learn (sklearn): the most widely used machine learning library.\n",
    "# We will import specific parts of it as we need them throughout the notebook.\n",
    "\n",
    "# This line makes our plots look nicer and appear inside the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# Set a consistent style for all our plots so they look clean.\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set a random seed so that every time you run this notebook,\n",
    "# you get the exact same results. This makes learning easier\n",
    "# because the numbers won't change between runs.\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version:  {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"\\nYou are ready to begin learning about Supervised Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SLIDE 2: What is Supervised Learning?\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 The Big Picture: What is Machine Learning?\n",
    "\n",
    "Before we define supervised learning, let us first understand what **machine learning (ML)** is.\n",
    "\n",
    "### Traditional Programming vs. Machine Learning\n",
    "\n",
    "In **traditional programming**, a human programmer writes explicit rules:\n",
    "\n",
    "```\n",
    "Traditional Programming:\n",
    "    INPUT (Data) + RULES (written by human) --> OUTPUT (Answers)\n",
    "    \n",
    "    Example: \"If temperature > 100°F, print 'hot'\"\n",
    "```\n",
    "\n",
    "In **machine learning**, the computer figures out the rules by looking at examples:\n",
    "\n",
    "```\n",
    "Machine Learning:\n",
    "    INPUT (Data) + OUTPUT (Answers) --> RULES (learned by computer)\n",
    "    \n",
    "    Example: Given thousands of temperatures labeled 'hot' or 'cold',\n",
    "             the computer learns the boundary on its own.\n",
    "```\n",
    "\n",
    "**Key insight**: In ML, we do NOT tell the computer the rules. Instead, we give it many examples and let it figure out the patterns by itself.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Definition of Supervised Learning\n",
    "\n",
    "> **Supervised Learning** is a machine learning paradigm where the model learns from **labeled data**.\n",
    "\n",
    "Let us break this definition down word by word:\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| **Supervised** | \"Supervised\" means there is a teacher (the labels). Just like a student learning with an answer key, the model has the correct answers during training. |\n",
    "| **Learning** | The model improves its predictions by looking at more and more examples. |\n",
    "| **Labeled data** | Each piece of data comes with a correct answer (a \"label\"). For example, a photo of a cat comes with the label \"cat\". |\n",
    "\n",
    "### Real-World Analogy: Learning with Flashcards\n",
    "\n",
    "Imagine you are studying for a vocabulary test using flashcards:\n",
    "\n",
    "- **Front of the card** (the input): A word in Spanish, e.g., \"gato\"\n",
    "- **Back of the card** (the label): The English translation, e.g., \"cat\"\n",
    "\n",
    "You study many flashcards (training data). Each card has both the question AND the answer. After studying enough cards, someone shows you a NEW Spanish word you have never seen, and you try to guess the English translation.\n",
    "\n",
    "That is EXACTLY what supervised learning does:\n",
    "1. **Training phase**: Study many examples where both the input and correct answer are known.\n",
    "2. **Prediction phase**: Given a new input (never seen before), predict the correct answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The Mathematical Notation: Data Samples\n",
    "\n",
    "In supervised learning, our data is organized as **pairs**:\n",
    "\n",
    "$$\\text{Data samples: } (x_i, y_i) \\text{ for } i = 1, 2, \\ldots, n$$\n",
    "\n",
    "Let us decode this notation:\n",
    "\n",
    "| Symbol | Name | Meaning | Example |\n",
    "|--------|------|---------|--------|\n",
    "| $x_i$ | **Input** (also called \"feature\") | The information we give to the model | A photo, a number, a sentence |\n",
    "| $y_i$ | **Output** (also called \"label\" or \"target\") | The correct answer for that input | \"cat\", 72.5, \"positive\" |\n",
    "| $i$ | **Index** | Which example we are looking at | 1st example, 2nd example, etc. |\n",
    "| $n$ | **Total count** | How many examples we have | 1000 training examples |\n",
    "| $(x_i, y_i)$ | **Data pair** | One input matched with its correct output | (photo_of_cat, \"cat\") |\n",
    "\n",
    "### Why Pairs?\n",
    "\n",
    "Each input $x_i$ is **paired** with its correct label $y_i$. This pairing is what makes it \"supervised\" -- the model can check its guesses against the true answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 2.3: Let's create labeled data in Python!\n",
    "# =============================================================================\n",
    "# Scenario: We have data about houses.\n",
    "# Input (x): the size of the house in square feet.\n",
    "# Output (y): the price of the house in thousands of dollars.\n",
    "#\n",
    "# Each (x_i, y_i) pair is one house with its known price.\n",
    "# =============================================================================\n",
    "\n",
    "# x_i values: house sizes in square feet\n",
    "# We use a numpy array because it is efficient for numerical data.\n",
    "x_inputs = np.array([600, 800, 1000, 1200, 1500, 1800, 2000, 2500, 3000, 3500])\n",
    "\n",
    "# y_i values: house prices in thousands of dollars\n",
    "# Each price corresponds to the house size at the same position.\n",
    "# For example, x_inputs[0]=600 sqft has y_labels[0]=$120k\n",
    "y_labels = np.array([120, 160, 200, 240, 310, 370, 420, 530, 620, 740])\n",
    "\n",
    "# Let's see how many data samples we have.\n",
    "# In our mathematical notation, this is 'n'.\n",
    "n = len(x_inputs)\n",
    "print(f\"Number of data samples (n): {n}\")\n",
    "print()\n",
    "\n",
    "# Let's display each (x_i, y_i) pair so you can see the structure.\n",
    "print(\"Our labeled data samples (x_i, y_i):\")\n",
    "print(\"=\" * 50)\n",
    "# Loop through each example using enumerate which gives us index i and the value.\n",
    "for i, (x_i, y_i) in enumerate(zip(x_inputs, y_labels)):\n",
    "    # i starts at 0 in Python, but we add 1 to match the math notation (1-indexed).\n",
    "    print(f\"  Sample {i+1:2d}:  x_{i+1} = {x_i:5d} sqft  -->  y_{i+1} = ${y_i:4d}k\")\n",
    "\n",
    "print()\n",
    "print(\"Notice: Every input x_i is PAIRED with a label y_i.\")\n",
    "print(\"This is what makes it LABELED data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 2.3b: Let's also display this data as a nice table using pandas.\n",
    "# =============================================================================\n",
    "\n",
    "# Create a pandas DataFrame, which is like a spreadsheet / table.\n",
    "# Each column has a name, and each row is one data sample.\n",
    "data_table = pd.DataFrame({\n",
    "    'Sample (i)': range(1, n + 1),           # Sample index: 1, 2, ..., n\n",
    "    'Input x_i (sqft)': x_inputs,             # The input feature\n",
    "    'Label y_i (price $k)': y_labels           # The correct output label\n",
    "})\n",
    "\n",
    "# Display the table. In Jupyter notebooks, DataFrames are rendered as nice HTML tables.\n",
    "print(\"Labeled Dataset as a Table:\")\n",
    "print()\n",
    "# .to_string(index=False) prints the table without the default pandas row index\n",
    "print(data_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 The Goal of Machine Learning\n",
    "\n",
    "> **Goal**: Given an **unseen** $x$ (an input the model has never seen before), predict the label $y$.\n",
    "\n",
    "This is the fundamental purpose of supervised learning. Let us break it down:\n",
    "\n",
    "1. **Training Phase**: The model studies the labeled data: $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$\n",
    "2. **Learning**: The model finds patterns in the data (e.g., \"bigger houses tend to cost more\")\n",
    "3. **Prediction Phase**: Someone gives the model a brand new $x_{\\text{new}}$ it has never seen\n",
    "4. **Output**: The model predicts $\\hat{y}$ (read as \"y-hat\"), its best guess for the label\n",
    "\n",
    "### Important Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $y$ | The **true** (correct) label |\n",
    "| $\\hat{y}$ | The **predicted** label (the model's guess). The hat ^ means \"estimate\". |\n",
    "\n",
    "A good model makes predictions $\\hat{y}$ that are very close to the true labels $y$.\n",
    "\n",
    "### Real-World Analogy: Studying for an Exam\n",
    "\n",
    "- **Training data** = Practice problems with answer keys\n",
    "- **Learning** = Studying those practice problems to understand the patterns\n",
    "- **Unseen x** = The actual exam questions (you have never seen these exact questions)\n",
    "- **Prediction** = Your answers on the exam\n",
    "- **Good model** = A well-prepared student who gets most exam answers right\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 2.4: Demonstrating the Goal of ML -- Predict unseen data\n",
    "# =============================================================================\n",
    "# We will use our house price data to show the concept.\n",
    "# The model learns from the 10 houses we showed it,\n",
    "# then predicts the price of a house it has NEVER seen.\n",
    "# =============================================================================\n",
    "\n",
    "# Import LinearRegression from scikit-learn.\n",
    "# This is one of the simplest ML models. Don't worry about the details yet;\n",
    "# we will explain models in more depth later. For now, just think of it as\n",
    "# a \"learning machine\" that finds patterns in data.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# STEP 1: Prepare the training data.\n",
    "# sklearn expects the input X to be a 2D array (a table with rows and columns),\n",
    "# even if we only have one feature. reshape(-1, 1) converts our 1D array\n",
    "# into a 2D array with one column.\n",
    "# Before reshape: [600, 800, 1000, ...] -- shape is (10,)\n",
    "# After  reshape: [[600], [800], [1000], ...] -- shape is (10, 1)\n",
    "X_train = x_inputs.reshape(-1, 1)\n",
    "y_train = y_labels\n",
    "\n",
    "print(\"STEP 1: Training data prepared\")\n",
    "print(f\"  X_train shape: {X_train.shape}  (10 samples, 1 feature each)\")\n",
    "print(f\"  y_train shape: {y_train.shape}  (10 labels)\")\n",
    "print()\n",
    "\n",
    "# STEP 2: Create the model and train it.\n",
    "# .fit() is the function that tells the model to LEARN from the data.\n",
    "# This is the \"training phase\" or \"learning phase\".\n",
    "model = LinearRegression()       # Create a new (untrained) model\n",
    "model.fit(X_train, y_train)      # Train the model on our labeled data\n",
    "\n",
    "print(\"STEP 2: Model has been trained (it studied our 10 labeled examples)\")\n",
    "print()\n",
    "\n",
    "# STEP 3: Predict on UNSEEN data.\n",
    "# Let's ask: what would a 1,750 sqft house cost?\n",
    "# The model has never seen a 1,750 sqft house in the training data!\n",
    "x_new = np.array([[1750]])  # New, unseen input (must be 2D for sklearn)\n",
    "\n",
    "# .predict() asks the model to make a prediction.\n",
    "y_predicted = model.predict(x_new)\n",
    "\n",
    "print(\"STEP 3: Prediction on UNSEEN data\")\n",
    "print(f\"  New input (x_new):    {x_new[0][0]} sqft\")\n",
    "print(f\"  Predicted price (y^): ${y_predicted[0]:.1f}k\")\n",
    "print()\n",
    "print(\"The model predicted a price for a house size it was NEVER trained on!\")\n",
    "print(\"This is the GOAL of machine learning: generalize to new, unseen data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 2.4: Plot the training data and the prediction\n",
    "# =============================================================================\n",
    "\n",
    "# Create a figure (the blank canvas) and axes (the plotting area).\n",
    "# figsize=(10, 6) means the plot will be 10 inches wide and 6 inches tall.\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the training data as blue circles.\n",
    "# 'o' means circle markers, 's' is markersize, 'zorder' controls layering.\n",
    "ax.scatter(x_inputs, y_labels, color='blue', s=100, zorder=5,\n",
    "           label='Training data (labeled)', edgecolors='black')\n",
    "\n",
    "# Plot the prediction as a red star.\n",
    "ax.scatter(x_new[0][0], y_predicted[0], color='red', s=300, zorder=6,\n",
    "           marker='*', label=f'Prediction for {x_new[0][0]} sqft = ${y_predicted[0]:.0f}k',\n",
    "           edgecolors='black')\n",
    "\n",
    "# Draw the line that the model learned (the pattern it found).\n",
    "# We create a range of x values and predict y for each one.\n",
    "x_line = np.linspace(400, 3800, 100).reshape(-1, 1)  # 100 points from 400 to 3800\n",
    "y_line = model.predict(x_line)                          # Model's prediction for each\n",
    "ax.plot(x_line, y_line, color='green', linewidth=2, linestyle='--',\n",
    "        label='Learned pattern (model)', alpha=0.7)\n",
    "\n",
    "# Add an arrow pointing to the prediction to highlight it.\n",
    "ax.annotate('NEW unseen input!',\n",
    "            xy=(x_new[0][0], y_predicted[0]),         # Point the arrow AT\n",
    "            xytext=(x_new[0][0] + 400, y_predicted[0] + 80),  # Text position\n",
    "            fontsize=12, color='red', fontweight='bold',\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "# Label the axes so the reader knows what each axis represents.\n",
    "ax.set_xlabel('House Size (sqft)', fontsize=14)\n",
    "ax.set_ylabel('House Price ($k)', fontsize=14)\n",
    "ax.set_title('The Goal of ML: Predict Labels for Unseen Inputs', fontsize=16)\n",
    "\n",
    "# Add a legend to explain the colors/symbols.\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "\n",
    "# Make the plot look clean.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBlue dots = training data the model LEARNED from.\")\n",
    "print(\"Green dashed line = the pattern the model discovered.\")\n",
    "print(\"Red star = prediction for new, UNSEEN data. That's the goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.5 Deep Learning: A Special Kind of Supervised Learning\n",
    "\n",
    "The lecture slide mentions:\n",
    "\n",
    "> **Deep Learning**: Input --> Neural Networks (input layer, hidden layers, output layer) --> Output\n",
    "\n",
    "### What is Deep Learning?\n",
    "\n",
    "Deep learning is a **subset** (a special case) of machine learning that uses **neural networks** -- computational systems loosely inspired by the human brain.\n",
    "\n",
    "```\n",
    "Artificial Intelligence (AI)\n",
    "  └── Machine Learning (ML)\n",
    "        └── Deep Learning (DL)  <-- uses neural networks\n",
    "```\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "A neural network is made up of **layers** of interconnected \"neurons\" (mathematical units):\n",
    "\n",
    "```\n",
    "Input Layer          Hidden Layer(s)         Output Layer\n",
    "  (x)                (processing)              (y-hat)\n",
    "                                              \n",
    "  [x1] ----\\        /--[ h1 ]--\\             \n",
    "             \\------/            \\--------->  [y-hat]\n",
    "  [x2] ----/--------\\--[ h2 ]--/\n",
    "                       \n",
    "```\n",
    "\n",
    "| Layer | Role | Analogy |\n",
    "|-------|------|---------|\n",
    "| **Input Layer** | Receives the raw data (features) | Your eyes seeing a photo |\n",
    "| **Hidden Layer(s)** | Processes and transforms the data, extracting patterns | Your brain analyzing what it sees |\n",
    "| **Output Layer** | Produces the final prediction | Your mouth saying \"that's a cat\" |\n",
    "\n",
    "### Why is it called \"Deep\"?\n",
    "\n",
    "- A network with **many hidden layers** is called a \"deep\" neural network.\n",
    "- More layers = the network can learn more complex patterns.\n",
    "- Example: A simple network might have 2-3 layers. A deep network (like those used in ChatGPT) can have hundreds of layers!\n",
    "\n",
    "### Real-World Analogy: A Factory Assembly Line\n",
    "\n",
    "Think of a neural network as a factory:\n",
    "\n",
    "- **Input layer** = Raw materials arriving at the factory (raw data)\n",
    "- **Hidden layers** = Workers at different stations who each do a specific job (processing, transforming, refining the data)\n",
    "- **Output layer** = The finished product that comes off the assembly line (the prediction)\n",
    "\n",
    "Each worker (neuron) takes the output from the previous worker, does some processing, and passes it to the next. The more workers (layers) you have, the more complex the product (prediction) you can make.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 2.5: Draw a simple neural network diagram\n",
    "# =============================================================================\n",
    "# We will draw a simple neural network with:\n",
    "#   - 3 input neurons (input layer)\n",
    "#   - 4 hidden neurons (hidden layer)\n",
    "#   - 1 output neuron (output layer)\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Define the x-position of each layer (left to right).\n",
    "layer_x = [0.15, 0.5, 0.85]  # Input layer at 0.15, hidden at 0.5, output at 0.85\n",
    "\n",
    "# Define the y-positions of neurons in each layer.\n",
    "# More neurons = more y-positions.\n",
    "input_neurons_y  = [0.25, 0.5, 0.75]           # 3 input neurons\n",
    "hidden_neurons_y = [0.15, 0.38, 0.62, 0.85]    # 4 hidden neurons\n",
    "output_neurons_y = [0.5]                         # 1 output neuron\n",
    "\n",
    "# Choose colors for each layer.\n",
    "input_color  = '#4CAF50'  # Green\n",
    "hidden_color = '#2196F3'  # Blue\n",
    "output_color = '#FF5722'  # Red-orange\n",
    "\n",
    "# --- Draw connections (lines) between neurons ---\n",
    "# Draw lines from every input neuron to every hidden neuron.\n",
    "for iy in input_neurons_y:\n",
    "    for hy in hidden_neurons_y:\n",
    "        # Each line represents a \"weight\" -- a number the model learns.\n",
    "        ax.plot([layer_x[0], layer_x[1]], [iy, hy],\n",
    "                color='gray', alpha=0.3, linewidth=1)\n",
    "\n",
    "# Draw lines from every hidden neuron to every output neuron.\n",
    "for hy in hidden_neurons_y:\n",
    "    for oy in output_neurons_y:\n",
    "        ax.plot([layer_x[1], layer_x[2]], [hy, oy],\n",
    "                color='gray', alpha=0.3, linewidth=1)\n",
    "\n",
    "# --- Draw neurons (circles) ---\n",
    "neuron_size = 800  # Size of each circle\n",
    "\n",
    "# Input layer neurons\n",
    "for i, iy in enumerate(input_neurons_y):\n",
    "    ax.scatter(layer_x[0], iy, s=neuron_size, color=input_color,\n",
    "               edgecolors='black', linewidth=2, zorder=5)\n",
    "    ax.text(layer_x[0], iy, f'x{i+1}', ha='center', va='center',\n",
    "            fontsize=14, fontweight='bold', color='white')\n",
    "\n",
    "# Hidden layer neurons\n",
    "for i, hy in enumerate(hidden_neurons_y):\n",
    "    ax.scatter(layer_x[1], hy, s=neuron_size, color=hidden_color,\n",
    "               edgecolors='black', linewidth=2, zorder=5)\n",
    "    ax.text(layer_x[1], hy, f'h{i+1}', ha='center', va='center',\n",
    "            fontsize=14, fontweight='bold', color='white')\n",
    "\n",
    "# Output layer neuron\n",
    "for i, oy in enumerate(output_neurons_y):\n",
    "    ax.scatter(layer_x[2], oy, s=neuron_size, color=output_color,\n",
    "               edgecolors='black', linewidth=2, zorder=5)\n",
    "    ax.text(layer_x[2], oy, r'$\\hat{y}$', ha='center', va='center',\n",
    "            fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "# --- Add layer labels ---\n",
    "ax.text(layer_x[0], 0.0, 'Input\\nLayer', ha='center', va='center',\n",
    "        fontsize=14, fontweight='bold', color=input_color)\n",
    "ax.text(layer_x[1], 0.0, 'Hidden\\nLayer', ha='center', va='center',\n",
    "        fontsize=14, fontweight='bold', color=hidden_color)\n",
    "ax.text(layer_x[2], 0.0, 'Output\\nLayer', ha='center', va='center',\n",
    "        fontsize=14, fontweight='bold', color=output_color)\n",
    "\n",
    "# --- Add arrows showing data flow ---\n",
    "ax.annotate('', xy=(0.08, 0.5), xytext=(-0.02, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "ax.text(-0.04, 0.5, 'Input\\n(x)', ha='center', va='center',\n",
    "        fontsize=13, fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(1.0, 0.5), xytext=(0.92, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='black'))\n",
    "ax.text(1.04, 0.5, 'Output\\n(prediction)', ha='center', va='center',\n",
    "        fontsize=13, fontweight='bold')\n",
    "\n",
    "# Remove axes since this is a diagram, not a data plot.\n",
    "ax.set_xlim(-0.12, 1.12)\n",
    "ax.set_ylim(-0.1, 1.0)\n",
    "ax.axis('off')\n",
    "ax.set_title('Deep Learning: Neural Network Architecture\\n(Input Layer --> Hidden Layer --> Output Layer)',\n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The diagram above shows a simple neural network.\")\n",
    "print(\"- Green nodes (x1, x2, x3): INPUT layer -- receives raw data\")\n",
    "print(\"- Blue nodes (h1-h4): HIDDEN layer -- processes and transforms data\")\n",
    "print(\"- Red node (y-hat): OUTPUT layer -- produces the final prediction\")\n",
    "print(\"- Gray lines: CONNECTIONS (weights) that the model learns during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary of Slide 2: What is Supervised Learning?\n",
    "\n",
    "| Concept | Key Takeaway |\n",
    "|---------|--------------|\n",
    "| Supervised Learning | A type of ML where the model learns from data that has known correct answers (labels). |\n",
    "| Data format | Pairs of (input, label): $(x_i, y_i)$ for $i = 1, \\ldots, n$ |\n",
    "| Input ($x_i$) | The information fed into the model (features) |\n",
    "| Output ($y_i$) | The correct answer (label/target) the model tries to learn |\n",
    "| Goal of ML | Given a NEW, unseen input $x$, predict its label $y$ accurately |\n",
    "| Deep Learning | Uses neural networks with input, hidden, and output layers |\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE 3: Examples of Supervised Learning\n",
    "\n",
    "---\n",
    "\n",
    "Supervised learning is used in a huge variety of real-world applications. The lecture slide lists the following examples. Let us go through each one in detail to understand what the **input** is, what the **label** is, and why it matters.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Comprehensive Table of Examples\n",
    "\n",
    "| # | Application | Input ($x$) | Label ($y$) | Industry | Why It Matters |\n",
    "|---|-------------|-------------|-------------|----------|----------------|\n",
    "| 1 | **Image Classification** | A photograph (pixels) | Category (\"cat\", \"dog\", \"car\") | Tech, Healthcare | Self-driving cars recognizing stop signs; doctors detecting tumors in X-rays |\n",
    "| 2 | **Document Categorization** | A text document | Category (\"sports\", \"politics\", \"science\") | Media, Legal | Automatically sorting news articles; organizing legal documents |\n",
    "| 3 | **Speech Recognition** | Audio waveform | Text transcript | Tech, Accessibility | Siri, Alexa, Google Assistant converting your voice to text |\n",
    "| 4 | **Protein Classification** | Protein structure/sequence | Protein family or function | Biotech, Pharma | Drug discovery; understanding diseases at the molecular level |\n",
    "| 5 | **Spam Detection** | An email (text, metadata) | \"spam\" or \"not spam\" | Email, Security | Filtering junk emails so your inbox stays clean |\n",
    "| 6 | **Branch Prediction** | CPU instruction history | Next branch direction (taken/not taken) | Computer Architecture | Making your computer's CPU run faster |\n",
    "| 7 | **Fraud Detection** | Credit card transaction | \"fraudulent\" or \"legitimate\" | Finance, Banking | Protecting your bank account from unauthorized charges |\n",
    "| 8 | **Natural Language Processing (NLP)** | Text (sentence, document) | Sentiment, translation, entity | Tech, Business | Chatbots, language translation, sentiment analysis of reviews |\n",
    "| 9 | **Playing Games** | Game state (board, score) | Best next move or outcome | AI Research, Entertainment | AlphaGo beating world champions at the game of Go |\n",
    "| 10 | **Computational Advertising** | User profile + webpage context | \"click\" or \"no click\" | Marketing, Tech | Showing you ads that are relevant to your interests |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Detailed Walkthrough of Selected Examples\n",
    "\n",
    "Let us dive deeper into a few of these examples to really solidify the concept.\n",
    "\n",
    "---\n",
    "\n",
    "### Example A: Image Classification\n",
    "\n",
    "**Problem**: Given a photograph, identify what object is in it.\n",
    "\n",
    "- **Input ($x$)**: An image, which to a computer is a grid of numbers (pixel values). A 28x28 grayscale image is 784 numbers.\n",
    "- **Label ($y$)**: The name of the object, e.g., \"cat\", \"dog\", \"airplane\".\n",
    "- **Training**: Show the model thousands of labeled images (\"this image is a cat\", \"this one is a dog\").\n",
    "- **Prediction**: Give the model a new photo it has never seen, and it tells you what it thinks it is.\n",
    "\n",
    "**Why it's supervised**: Each training image comes WITH its correct label.\n",
    "\n",
    "### Example B: Spam Detection\n",
    "\n",
    "**Problem**: Is this email spam or not?\n",
    "\n",
    "- **Input ($x$)**: Features of the email -- word frequencies, sender address, presence of links, etc.\n",
    "- **Label ($y$)**: \"spam\" or \"not spam\" (also called \"ham\").\n",
    "- **Training**: Show the model thousands of emails that humans have already labeled as spam or not spam.\n",
    "- **Prediction**: When a new email arrives, the model classifies it instantly.\n",
    "\n",
    "**Why it's supervised**: Humans provided the correct labels for the training emails.\n",
    "\n",
    "### Example C: Fraud Detection\n",
    "\n",
    "**Problem**: Is this credit card transaction fraudulent?\n",
    "\n",
    "- **Input ($x$)**: Transaction details -- amount, time, location, merchant type, whether card was present.\n",
    "- **Label ($y$)**: \"fraudulent\" or \"legitimate\".\n",
    "- **Training**: The model studies millions of past transactions where investigators already determined if fraud occurred.\n",
    "- **Prediction**: When a new transaction happens, the model flags it in real-time if it looks suspicious.\n",
    "\n",
    "**Why it's supervised**: Historical transactions were labeled by fraud investigators.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Analogy: A Doctor Learning to Diagnose\n",
    "\n",
    "Think of all these examples like a medical student learning to diagnose diseases:\n",
    "\n",
    "1. **Training** (medical school): The student studies thousands of patient cases where the diagnosis is already known. X-ray showing a dark spot? The professor says \"that's a tumor.\" (labeled data)\n",
    "2. **Practice** (residency): The student gets better and better at recognizing patterns.\n",
    "3. **Real world** (practicing doctor): A new patient walks in with a new X-ray. The doctor uses their training to predict the diagnosis.\n",
    "\n",
    "Every single supervised learning application follows this same pattern: learn from labeled examples, then predict on new data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 3.2: Let's build a real Spam Detection classifier!\n",
    "# =============================================================================\n",
    "# We will create a tiny (but real) spam detector using made-up email features.\n",
    "# This demonstrates the concepts from Slide 3 with actual running code.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Step 1: Create our labeled training data ---\n",
    "# We simulate email features:\n",
    "#   Feature 1: Number of exclamation marks in the email\n",
    "#   Feature 2: Number of links in the email\n",
    "#   Feature 3: Number of times \"free\" appears\n",
    "#   Feature 4: Length of the email (number of words)\n",
    "#\n",
    "# Label: 1 = spam, 0 = not spam (ham)\n",
    "\n",
    "# Each row is one email: [exclamation_marks, links, \"free\"_count, word_count]\n",
    "email_features = np.array([\n",
    "    # --- Spam emails (label = 1) ---\n",
    "    [15, 8, 5, 50],    # Many exclamation marks, lots of links, says \"free\" a lot\n",
    "    [12, 6, 4, 40],\n",
    "    [20, 10, 7, 30],\n",
    "    [18, 9, 6, 45],\n",
    "    [10, 7, 3, 35],\n",
    "    [25, 12, 8, 20],\n",
    "    [14, 5, 4, 55],\n",
    "    [22, 11, 9, 25],\n",
    "    # --- Legitimate (ham) emails (label = 0) ---\n",
    "    [1, 1, 0, 200],    # Few exclamation marks, few links, no \"free\", longer email\n",
    "    [0, 0, 0, 150],\n",
    "    [2, 2, 0, 300],\n",
    "    [1, 0, 0, 180],\n",
    "    [3, 1, 1, 250],\n",
    "    [0, 1, 0, 120],\n",
    "    [2, 0, 0, 400],\n",
    "    [1, 2, 0, 350],\n",
    "])\n",
    "\n",
    "# Labels: 1 = spam, 0 = not spam\n",
    "# The first 8 emails are spam, the last 8 are legitimate.\n",
    "email_labels = np.array([1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                         0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Let's see the data in a nice table.\n",
    "email_df = pd.DataFrame(email_features,\n",
    "                        columns=['Exclamation Marks', 'Links', '\"Free\" Count', 'Word Count'])\n",
    "email_df['Label'] = ['SPAM' if l == 1 else 'HAM' for l in email_labels]\n",
    "\n",
    "print(\"Our Labeled Email Dataset:\")\n",
    "print(\"=\" * 65)\n",
    "print(email_df.to_string(index=True))\n",
    "print()\n",
    "print(f\"Total emails: {len(email_labels)}\")\n",
    "print(f\"Spam emails:  {sum(email_labels == 1)}\")\n",
    "print(f\"Ham emails:   {sum(email_labels == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 3.2b: Train a spam classifier and make predictions\n",
    "# =============================================================================\n",
    "\n",
    "# Import a simple classifier: Decision Tree.\n",
    "# A decision tree makes predictions by asking yes/no questions about the data.\n",
    "# Example: \"Does the email have > 5 exclamation marks? If yes -> likely spam\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and train the model.\n",
    "spam_model = DecisionTreeClassifier(random_state=42)  # Create the model\n",
    "spam_model.fit(email_features, email_labels)           # Train on labeled data\n",
    "\n",
    "print(\"Spam detection model has been trained!\")\n",
    "print()\n",
    "\n",
    "# --- Now let's predict on NEW, UNSEEN emails ---\n",
    "# Email A: Looks like spam (many exclamation marks, links, says \"free\")\n",
    "new_email_A = np.array([[16, 7, 5, 35]])\n",
    "# Email B: Looks legitimate (few exclamation marks, no \"free\", long email)\n",
    "new_email_B = np.array([[1, 1, 0, 280]])\n",
    "# Email C: Ambiguous case\n",
    "new_email_C = np.array([[5, 3, 1, 100]])\n",
    "\n",
    "# Make predictions. predict() returns 0 or 1.\n",
    "pred_A = spam_model.predict(new_email_A)[0]\n",
    "pred_B = spam_model.predict(new_email_B)[0]\n",
    "pred_C = spam_model.predict(new_email_C)[0]\n",
    "\n",
    "# Display predictions.\n",
    "print(\"Predictions on UNSEEN emails:\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"  Email A (16 !, 7 links, 5 'free', 35 words): {'SPAM' if pred_A else 'HAM'}\")\n",
    "print(f\"  Email B ( 1 !, 1 link,  0 'free', 280 words): {'SPAM' if pred_B else 'HAM'}\")\n",
    "print(f\"  Email C ( 5 !, 3 links, 1 'free', 100 words): {'SPAM' if pred_C else 'HAM'}\")\n",
    "print()\n",
    "print(\"The model used patterns from the LABELED training emails to classify\")\n",
    "print(\"these NEW emails it had never seen before. This is supervised learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 3.2: Visualize spam vs ham emails\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Plot 1: Exclamation marks vs Links ---\n",
    "ax1 = axes[0]\n",
    "# Separate spam and ham for coloring.\n",
    "spam_mask = email_labels == 1  # Boolean mask: True for spam rows\n",
    "ham_mask  = email_labels == 0  # Boolean mask: True for ham rows\n",
    "\n",
    "# Plot spam emails as red X markers.\n",
    "ax1.scatter(email_features[spam_mask, 0], email_features[spam_mask, 1],\n",
    "            color='red', marker='x', s=150, linewidth=3, label='Spam', zorder=5)\n",
    "# Plot ham emails as blue circle markers.\n",
    "ax1.scatter(email_features[ham_mask, 0], email_features[ham_mask, 1],\n",
    "            color='blue', marker='o', s=100, label='Ham (not spam)', zorder=5,\n",
    "            edgecolors='black')\n",
    "\n",
    "ax1.set_xlabel('Number of Exclamation Marks', fontsize=12)\n",
    "ax1.set_ylabel('Number of Links', fontsize=12)\n",
    "ax1.set_title('Spam vs Ham: Exclamation Marks vs Links', fontsize=13)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# --- Plot 2: \"Free\" count vs Word count ---\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(email_features[spam_mask, 2], email_features[spam_mask, 3],\n",
    "            color='red', marker='x', s=150, linewidth=3, label='Spam', zorder=5)\n",
    "ax2.scatter(email_features[ham_mask, 2], email_features[ham_mask, 3],\n",
    "            color='blue', marker='o', s=100, label='Ham (not spam)', zorder=5,\n",
    "            edgecolors='black')\n",
    "\n",
    "ax2.set_xlabel('\"Free\" Word Count', fontsize=12)\n",
    "ax2.set_ylabel('Email Length (words)', fontsize=12)\n",
    "ax2.set_title('Spam vs Ham: \"Free\" Count vs Email Length', fontsize=13)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice the clear separation between spam (red X) and ham (blue dots):\")\n",
    "print(\"  - Spam emails tend to have MORE exclamation marks, links, and 'free' mentions\")\n",
    "print(\"  - Legitimate emails tend to be LONGER and have fewer spammy features\")\n",
    "print(\"  - The ML model learns these patterns automatically from labeled data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary of Slide 3: Examples of Supervised Learning\n",
    "\n",
    "| Key Insight | Explanation |\n",
    "|-------------|-------------|\n",
    "| Supervised learning is everywhere | From email (spam detection) to healthcare (protein classification) to finance (fraud detection) |\n",
    "| The pattern is always the same | Collect labeled data, train a model, predict on new data |\n",
    "| Different inputs, same idea | Whether the input is an image, text, audio, or numbers, the supervised learning framework is identical |\n",
    "| Labels are the key | What makes it \"supervised\" is that we have correct answers to learn from |\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE 4: Supervised Learning Workflow\n",
    "\n",
    "---\n",
    "\n",
    "Now that we understand WHAT supervised learning is and WHERE it is used, let us understand HOW it works step by step.\n",
    "\n",
    "## 4.1 The Workflow Diagram\n",
    "\n",
    "The lecture slide shows this workflow:\n",
    "\n",
    "```\n",
    "                 TRAINING PHASE\n",
    "    ┌──────────────────────────────────────┐\n",
    "    │                                      │\n",
    "    │  Labeled Data ──┐                    │\n",
    "    │                 ├──> Machine ──> ML Model\n",
    "    │  Labels ────────┘   (Learning         │\n",
    "    │                      Algorithm)       │\n",
    "    └──────────────────────────────────────┘\n",
    "                                            \n",
    "                 PREDICTION PHASE            \n",
    "    ┌──────────────────────────────────────┐\n",
    "    │                                      │\n",
    "    │  Test Data ──> ML Model ──> Predictions\n",
    "    │                (trained)              │\n",
    "    └──────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 4.2 Each Step Explained in Detail\n",
    "\n",
    "### Step 1: Labeled Data + Labels\n",
    "\n",
    "- **What**: We gather a dataset where every example has a known correct answer.\n",
    "- **Example**: 10,000 emails, each labeled as \"spam\" or \"not spam\" by a human.\n",
    "- **Why this step matters**: Without labeled data, the model has nothing to learn from. This is often the most time-consuming and expensive step!\n",
    "\n",
    "### Step 2: Machine (Learning Algorithm)\n",
    "\n",
    "- **What**: A mathematical algorithm that examines the labeled data and discovers patterns.\n",
    "- **Example**: The algorithm notices that emails with many exclamation marks and the word \"free\" tend to be spam.\n",
    "- **Why this step matters**: This is where the actual \"learning\" happens. Different algorithms find patterns in different ways.\n",
    "\n",
    "### Step 3: ML Model (The Trained Model)\n",
    "\n",
    "- **What**: The result of training. It is a mathematical function that can make predictions.\n",
    "- **Example**: A function that takes email features as input and outputs \"spam\" or \"not spam\".\n",
    "- **Why this step matters**: The model encodes everything the algorithm learned. It can be saved and reused.\n",
    "\n",
    "### Step 4: Test Data (New, Unseen Data)\n",
    "\n",
    "- **What**: New examples that the model has NEVER seen during training.\n",
    "- **Example**: New emails arriving in your inbox right now.\n",
    "- **Why this step matters**: This is the real test -- can the model generalize to data it was not trained on?\n",
    "\n",
    "### Step 5: Predictions\n",
    "\n",
    "- **What**: The model's output for the test data.\n",
    "- **Example**: \"This new email is spam\" or \"This new email is not spam\".\n",
    "- **Why this step matters**: The whole point of the process! We evaluate how accurate these predictions are.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Analogy: A Cooking School\n",
    "\n",
    "| ML Workflow Step | Cooking Analogy |\n",
    "|-----------------|----------------|\n",
    "| Labeled Data + Labels | Recipe books with photos (input = ingredients, label = dish name) |\n",
    "| Machine (Learning Algorithm) | The cooking school curriculum (the process of learning) |\n",
    "| ML Model | The trained chef (has internalized the patterns) |\n",
    "| Test Data | New ingredients the chef has never used before |\n",
    "| Predictions | The dish the chef creates from new ingredients |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 4.1: Draw the Supervised Learning Workflow\n",
    "# =============================================================================\n",
    "# We will create a diagram that matches the slide's workflow.\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Helper function to draw a box with text.\n",
    "def draw_box(ax, x, y, width, height, text, color, fontsize=12, text_color='white'):\n",
    "    \"\"\"Draws a rounded rectangle with centered text.\"\"\"\n",
    "    import matplotlib.patches as mpatches\n",
    "    # FancyBboxPatch creates a box with rounded corners.\n",
    "    box = mpatches.FancyBboxPatch(\n",
    "        (x - width/2, y - height/2), width, height,\n",
    "        boxstyle=\"round,pad=0.1\", facecolor=color,\n",
    "        edgecolor='black', linewidth=2\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, text, ha='center', va='center',\n",
    "            fontsize=fontsize, fontweight='bold', color=text_color)\n",
    "\n",
    "# --- TRAINING PHASE (top half) ---\n",
    "ax.text(0.5, 0.95, 'TRAINING PHASE', ha='center', va='center',\n",
    "        fontsize=18, fontweight='bold', color='darkblue',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='darkblue', linewidth=2))\n",
    "\n",
    "draw_box(ax, 0.12, 0.78, 0.18, 0.1, 'Labeled\\nData', '#4CAF50')\n",
    "draw_box(ax, 0.12, 0.62, 0.18, 0.1, 'Labels', '#4CAF50')\n",
    "draw_box(ax, 0.42, 0.70, 0.2, 0.12, 'Learning\\nAlgorithm', '#2196F3')\n",
    "draw_box(ax, 0.75, 0.70, 0.18, 0.12, 'ML\\nModel', '#FF9800')\n",
    "\n",
    "# Arrows for training phase.\n",
    "ax.annotate('', xy=(0.31, 0.74), xytext=(0.22, 0.78),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='black'))\n",
    "ax.annotate('', xy=(0.31, 0.66), xytext=(0.22, 0.62),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='black'))\n",
    "ax.annotate('', xy=(0.65, 0.70), xytext=(0.53, 0.70),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='black'))\n",
    "\n",
    "# --- PREDICTION PHASE (bottom half) ---\n",
    "ax.text(0.5, 0.42, 'PREDICTION PHASE', ha='center', va='center',\n",
    "        fontsize=18, fontweight='bold', color='darkred',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='darkred', linewidth=2))\n",
    "\n",
    "draw_box(ax, 0.15, 0.25, 0.18, 0.1, 'Test\\nData', '#9C27B0')\n",
    "draw_box(ax, 0.48, 0.25, 0.22, 0.12, 'Trained\\nML Model', '#FF9800')\n",
    "draw_box(ax, 0.82, 0.25, 0.2, 0.1, 'Predictions', '#F44336')\n",
    "\n",
    "# Arrows for prediction phase.\n",
    "ax.annotate('', xy=(0.36, 0.25), xytext=(0.25, 0.25),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='black'))\n",
    "ax.annotate('', xy=(0.71, 0.25), xytext=(0.60, 0.25),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2.5, color='black'))\n",
    "\n",
    "# Arrow connecting training to prediction (the model is reused).\n",
    "ax.annotate('', xy=(0.53, 0.32), xytext=(0.75, 0.63),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='gray',\n",
    "                           linestyle='dashed'))\n",
    "ax.text(0.72, 0.48, 'Model is\\nreused', ha='center', va='center',\n",
    "        fontsize=10, color='gray', fontstyle='italic')\n",
    "\n",
    "# Clean up.\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0.1, 1.05)\n",
    "ax.axis('off')\n",
    "ax.set_title('Supervised Learning Workflow (Slide 4)', fontsize=20, fontweight='bold', pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 4.2: Full supervised learning workflow in code\n",
    "# =============================================================================\n",
    "# We will demonstrate the COMPLETE workflow end-to-end using\n",
    "# the famous Iris dataset (classifying flowers based on measurements).\n",
    "# =============================================================================\n",
    "\n",
    "# Import the dataset and tools we need.\n",
    "from sklearn.datasets import load_iris                  # A famous labeled dataset\n",
    "from sklearn.model_selection import train_test_split     # Splits data into train/test\n",
    "from sklearn.neighbors import KNeighborsClassifier       # A simple ML algorithm\n",
    "from sklearn.metrics import accuracy_score               # Measures prediction quality\n",
    "\n",
    "# ---- STEP 1: Get labeled data ----\n",
    "# The Iris dataset has measurements of 150 iris flowers.\n",
    "# Each flower is labeled as one of 3 species: setosa, versicolor, virginica.\n",
    "iris = load_iris()\n",
    "\n",
    "# X = inputs (features): sepal length, sepal width, petal length, petal width\n",
    "X = iris.data\n",
    "# y = labels (targets): 0 = setosa, 1 = versicolor, 2 = virginica\n",
    "y = iris.target\n",
    "\n",
    "print(\"==== STEP 1: Labeled Data ====\")\n",
    "print(f\"Total number of samples: {len(X)}\")\n",
    "print(f\"Number of features per sample: {X.shape[1]}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Class names: {list(iris.target_names)}\")\n",
    "print(f\"\\nFirst 5 samples (inputs):\")\n",
    "for i in range(5):\n",
    "    print(f\"  x_{i+1} = {X[i]}  -->  y_{i+1} = {iris.target_names[y[i]]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 2: Split data into training set and test set ----\n",
    "# WHY split? We need to keep some data the model has NEVER seen,\n",
    "# so we can honestly evaluate how well it generalizes.\n",
    "#\n",
    "# train_test_split randomly divides the data:\n",
    "#   - 80% for training (the model learns from these)\n",
    "#   - 20% for testing (the model has never seen these)\n",
    "#\n",
    "# test_size=0.2 means 20% goes to the test set.\n",
    "# random_state=42 ensures reproducible results.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"==== STEP 2: Split into Training and Test Sets ====\")\n",
    "print(f\"Training samples: {len(X_train)} (used to LEARN)\")\n",
    "print(f\"Test samples:     {len(X_test)} (used to EVALUATE, model never sees these)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 3: Choose a learning algorithm and train the model ----\n",
    "# We use K-Nearest Neighbors (KNN), one of the simplest ML algorithms.\n",
    "# KNN works by finding the K closest training examples to a new input,\n",
    "# and predicting the most common label among those neighbors.\n",
    "#\n",
    "# Think of it like this: if you don't know what a flower is,\n",
    "# look at the 5 most similar flowers you've seen before, and go with\n",
    "# whatever species they mostly are.\n",
    "\n",
    "# Create the model with k=5 (look at 5 nearest neighbors).\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model. .fit() is the standard method name for training in sklearn.\n",
    "# This step is where the \"Machine\" (learning algorithm) processes the\n",
    "# \"Labeled Data + Labels\" from the workflow diagram.\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"==== STEP 3: Learning Algorithm --> ML Model ====\")\n",
    "print(\"Algorithm: K-Nearest Neighbors (k=5)\")\n",
    "print(\"The model has been trained. It has studied the 120 training examples.\")\n",
    "print(\"It is now ready to make predictions on new data.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 4: Feed test data to the trained model ----\n",
    "# The test data (X_test) represents new, unseen flowers.\n",
    "# The model predicts the species for each one.\n",
    "\n",
    "# .predict() makes predictions for all test samples at once.\n",
    "y_predicted = knn_model.predict(X_test)\n",
    "\n",
    "print(\"==== STEP 4: Test Data --> Trained ML Model --> Predictions ====\")\n",
    "print()\n",
    "print(f\"{'Sample':<8} {'True Label':<15} {'Predicted Label':<18} {'Correct?':<10}\")\n",
    "print(\"=\" * 55)\n",
    "# Show predictions for the first 15 test samples.\n",
    "for i in range(min(15, len(y_test))):\n",
    "    true_name = iris.target_names[y_test[i]]       # The actual species\n",
    "    pred_name = iris.target_names[y_predicted[i]]   # The model's guess\n",
    "    correct = \"Yes\" if y_test[i] == y_predicted[i] else \"No\"\n",
    "    print(f\"  {i+1:<6} {true_name:<15} {pred_name:<18} {correct:<10}\")\n",
    "print(\"  ...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 5: Evaluate the predictions ----\n",
    "# We compare the model's predictions (y_predicted) to the actual labels (y_test).\n",
    "# accuracy_score calculates what percentage of predictions are correct.\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "\n",
    "print(\"==== STEP 5: Evaluate Predictions ====\")\n",
    "print(f\"Number of correct predictions: {sum(y_test == y_predicted)} out of {len(y_test)}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}%\")\n",
    "print()\n",
    "if accuracy >= 0.9:\n",
    "    print(\"Excellent! The model correctly classified most unseen flowers.\")\n",
    "    print(\"This shows the model GENERALIZED well from the training data.\")\n",
    "else:\n",
    "    print(\"The model could use improvement, but the workflow concept is the same.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE WORKFLOW RECAP:\")\n",
    "print(\"  1. Labeled Data: 150 iris flowers with known species\")\n",
    "print(\"  2. Split: 120 for training, 30 for testing\")\n",
    "print(\"  3. Machine: K-Nearest Neighbors algorithm learns patterns\")\n",
    "print(\"  4. ML Model: The trained KNN model\")\n",
    "print(\"  5. Test Data --> Model --> Predictions (evaluated for accuracy)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 4.2: Visualize the Iris dataset and predictions\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# --- Plot 1: Training data (what the model learned from) ---\n",
    "ax1 = axes[0]\n",
    "# We plot using the first two features: sepal length (column 0) and sepal width (column 1)\n",
    "colors = ['green', 'blue', 'orange']\n",
    "for class_idx in range(3):\n",
    "    # Select rows where the label equals class_idx\n",
    "    mask = y_train == class_idx\n",
    "    ax1.scatter(X_train[mask, 0], X_train[mask, 1],\n",
    "               color=colors[class_idx], label=iris.target_names[class_idx],\n",
    "               s=80, edgecolors='black', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel(iris.feature_names[0], fontsize=12)\n",
    "ax1.set_ylabel(iris.feature_names[1], fontsize=12)\n",
    "ax1.set_title('Training Data (Model Learned From These)', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# --- Plot 2: Test data predictions ---\n",
    "ax2 = axes[1]\n",
    "for class_idx in range(3):\n",
    "    # Color by PREDICTED labels to show model's predictions.\n",
    "    mask = y_predicted == class_idx\n",
    "    ax2.scatter(X_test[mask, 0], X_test[mask, 1],\n",
    "               color=colors[class_idx], label=iris.target_names[class_idx],\n",
    "               s=80, edgecolors='black', alpha=0.7)\n",
    "\n",
    "# Mark incorrect predictions with red X markers.\n",
    "incorrect_mask = y_test != y_predicted\n",
    "if any(incorrect_mask):\n",
    "    ax2.scatter(X_test[incorrect_mask, 0], X_test[incorrect_mask, 1],\n",
    "               color='red', marker='x', s=200, linewidth=3,\n",
    "               label='WRONG prediction', zorder=10)\n",
    "\n",
    "ax2.set_xlabel(iris.feature_names[0], fontsize=12)\n",
    "ax2.set_ylabel(iris.feature_names[1], fontsize=12)\n",
    "ax2.set_title('Test Data Predictions (Model Never Saw These)', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: The training data the model learned from (labeled examples).\")\n",
    "print(\"Right: Predictions on new, unseen test data.\")\n",
    "print(\"  Colors show predicted species. Red X marks any wrong predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary of Slide 4: Supervised Learning Workflow\n",
    "\n",
    "| Step | What Happens | Code Equivalent |\n",
    "|------|-------------|----------------|\n",
    "| 1. Collect Labeled Data | Gather examples with known correct answers | `X, y = load_iris().data, load_iris().target` |\n",
    "| 2. Split Data | Separate into training and test sets | `train_test_split(X, y)` |\n",
    "| 3. Train (Learn) | Algorithm studies training data, finds patterns | `model.fit(X_train, y_train)` |\n",
    "| 4. Predict | Feed new data to the trained model | `y_pred = model.predict(X_test)` |\n",
    "| 5. Evaluate | Compare predictions to true answers | `accuracy_score(y_test, y_pred)` |\n",
    "\n",
    "**Key takeaway**: Every supervised learning project follows this same 5-step workflow, regardless of the application.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE 5: Types of Supervised Learning\n",
    "\n",
    "---\n",
    "\n",
    "There are two main types of supervised learning, based on what kind of thing we are trying to predict:\n",
    "\n",
    "## 5.1 The Two Types at a Glance\n",
    "\n",
    "| | **Regression** | **Classification** |\n",
    "|--|----------------|--------------------|\n",
    "| **What we predict** | A **continuous** number | A **discrete** category/label |\n",
    "| **Output type** | Any real number (e.g., 72.5, 84.0, -3.14) | A category from a fixed set (e.g., \"hot\", \"cold\") |\n",
    "| **Example question** | \"What will the temperature be tomorrow?\" | \"Will it be hot or cold tomorrow?\" |\n",
    "| **Example answer** | **84 degrees Fahrenheit** | **Hot** |\n",
    "| **Think of it as** | Predicting \"how much\" or \"how many\" | Predicting \"which one\" or \"what type\" |\n",
    "| **Output is on a** | Number line (infinite possibilities) | List of options (finite possibilities) |\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Regression: Predicting Continuous Values\n",
    "\n",
    "> **Regression** predicts a **continuous numerical value**. The output can be any number on a continuous scale.\n",
    "\n",
    "### What does \"continuous\" mean?\n",
    "\n",
    "A continuous value can take **any** value within a range, including decimals. There are infinitely many possible outputs.\n",
    "\n",
    "- Temperature: 72.0, 72.1, 72.15, 72.153, ... (infinite possibilities)\n",
    "- House price: $150,000, $150,001, $150,000.50, ...\n",
    "- A person's height: 5.5 feet, 5.51 feet, 5.512 feet, ...\n",
    "\n",
    "### Examples of Regression Problems\n",
    "\n",
    "| Problem | Input | Output (continuous) |\n",
    "|---------|-------|--------------------|\n",
    "| House price prediction | Size, location, bedrooms | Price in dollars |\n",
    "| Temperature forecasting | Past temperatures, humidity | Temperature in °F |\n",
    "| Stock price prediction | Historical prices, news | Price per share |\n",
    "| Age estimation | Photo of a face | Age in years |\n",
    "| Sales forecasting | Past sales, season, ads | Revenue in dollars |\n",
    "\n",
    "### Lecture Slide Example\n",
    "\n",
    "From the slide: \"What will the temperature be tomorrow?\" Answer: **84°F**\n",
    "\n",
    "This is regression because 84 is a specific number on a continuous scale. It could have been 83.5 or 84.2 -- any value is possible.\n",
    "\n",
    "### Real-World Analogy: A Thermometer\n",
    "\n",
    "Regression is like reading a thermometer -- the mercury can stop at ANY point along the scale. It is not limited to specific fixed marks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 5.2: Regression -- Predicting Temperature\n",
    "# =============================================================================\n",
    "# Let's build a regression model that predicts tomorrow's temperature\n",
    "# based on today's temperature and humidity.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Step 1: Create labeled data ---\n",
    "# Each sample: [today's temperature (°F), humidity (%)]\n",
    "# Label: tomorrow's temperature (°F) -- a CONTINUOUS value\n",
    "\n",
    "# We generate synthetic (made-up but realistic) data.\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "n_samples = 100  # 100 days of historical weather data\n",
    "\n",
    "# Today's temperature: random values between 30°F and 100°F\n",
    "todays_temp = np.random.uniform(30, 100, n_samples)\n",
    "\n",
    "# Humidity: random values between 20% and 90%\n",
    "humidity = np.random.uniform(20, 90, n_samples)\n",
    "\n",
    "# Tomorrow's temperature (the label):\n",
    "# In reality this is complex, but we simulate it as:\n",
    "# tomorrow = 0.85 * today + 0.05 * humidity + some randomness\n",
    "tomorrows_temp = 0.85 * todays_temp + 0.05 * humidity + np.random.normal(0, 5, n_samples)\n",
    "\n",
    "# Combine features into a 2D array (each row = [today_temp, humidity]).\n",
    "X_weather = np.column_stack([todays_temp, humidity])\n",
    "y_weather = tomorrows_temp\n",
    "\n",
    "print(\"Weather Regression Dataset:\")\n",
    "print(f\"  Number of samples: {n_samples}\")\n",
    "print(f\"  Features: today's temperature, humidity\")\n",
    "print(f\"  Label: tomorrow's temperature (continuous!)\")\n",
    "print()\n",
    "print(\"First 10 samples:\")\n",
    "print(f\"{'Today Temp (°F)':<18} {'Humidity (%)':<15} {'Tomorrow Temp (°F)':<20}\")\n",
    "print(\"=\" * 55)\n",
    "for i in range(10):\n",
    "    print(f\"  {todays_temp[i]:>8.1f}          {humidity[i]:>8.1f}        {tomorrows_temp[i]:>8.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"Notice: Tomorrow's temperature is a CONTINUOUS number (not a category).\")\n",
    "print(\"It can be 72.3, 84.7, 56.1 -- any value. This is a REGRESSION problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 5.2b: Train a regression model and visualize\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split data: 80% training, 20% testing.\n",
    "X_w_train, X_w_test, y_w_train, y_w_test = train_test_split(\n",
    "    X_weather, y_weather, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train a linear regression model.\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_w_train, y_w_train)\n",
    "\n",
    "# Make predictions on the test set.\n",
    "y_w_pred = reg_model.predict(X_w_test)\n",
    "\n",
    "# Show some predictions vs actual values.\n",
    "print(\"Regression Predictions vs Actual Values:\")\n",
    "print(f\"{'Actual Temp (°F)':<20} {'Predicted Temp (°F)':<22} {'Difference':<12}\")\n",
    "print(\"=\" * 55)\n",
    "for i in range(10):\n",
    "    diff = y_w_pred[i] - y_w_test[i]\n",
    "    print(f\"  {y_w_test[i]:>10.1f}          {y_w_pred[i]:>10.1f}           {diff:>+6.1f}\")\n",
    "\n",
    "# Calculate the mean squared error (how far off predictions are on average).\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_w_test, y_w_pred)\n",
    "r2 = r2_score(y_w_test, y_w_pred)\n",
    "\n",
    "print()\n",
    "print(f\"Mean Squared Error: {mse:.2f} (lower is better)\")\n",
    "print(f\"R-squared Score:    {r2:.3f} (closer to 1.0 is better)\")\n",
    "print()\n",
    "print(\"The model predicts a CONTINUOUS number (e.g., 84.3°F), not a category.\")\n",
    "print(\"This is what makes it REGRESSION.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 5.2: Regression results\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Plot 1: Today's temp vs Tomorrow's temp (with regression line) ---\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(todays_temp, tomorrows_temp, color='steelblue', alpha=0.5, s=40,\n",
    "            edgecolors='navy', label='Data points')\n",
    "\n",
    "# Draw the trend line.\n",
    "x_line_temp = np.linspace(25, 105, 100).reshape(-1, 1)\n",
    "# For the line, we use average humidity.\n",
    "x_line_with_humidity = np.column_stack([x_line_temp, np.full(100, humidity.mean())])\n",
    "y_line_temp = reg_model.predict(x_line_with_humidity)\n",
    "ax1.plot(x_line_temp, y_line_temp, color='red', linewidth=2.5, label='Regression line')\n",
    "\n",
    "ax1.set_xlabel(\"Today's Temperature (°F)\", fontsize=13)\n",
    "ax1.set_ylabel(\"Tomorrow's Temperature (°F)\", fontsize=13)\n",
    "ax1.set_title('REGRESSION: Predicting Continuous Values', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# --- Plot 2: Predicted vs Actual (perfect predictions would lie on the diagonal) ---\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_w_test, y_w_pred, color='steelblue', s=60, edgecolors='navy', alpha=0.7)\n",
    "\n",
    "# Draw the \"perfect prediction\" line (where predicted = actual).\n",
    "min_val = min(y_w_test.min(), y_w_pred.min()) - 5\n",
    "max_val = max(y_w_test.max(), y_w_pred.max()) + 5\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
    "\n",
    "ax2.set_xlabel('Actual Temperature (°F)', fontsize=13)\n",
    "ax2.set_ylabel('Predicted Temperature (°F)', fontsize=13)\n",
    "ax2.set_title('Predicted vs Actual (closer to red line = better)', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: How today's temperature relates to tomorrow's (the regression pattern).\")\n",
    "print(\"Right: Model's predictions vs reality. Points on the red line = perfect.\")\n",
    "print(\"\\nKey point: The output is a CONTINUOUS NUMBER, not a category.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.3 Classification: Predicting Discrete Labels\n",
    "\n",
    "> **Classification** predicts a **discrete category** (also called a \"class\" or \"label\"). The output is one choice from a fixed set of options.\n",
    "\n",
    "### What does \"discrete\" mean?\n",
    "\n",
    "A discrete value comes from a **fixed, countable set** of options. There is no \"in between\".\n",
    "\n",
    "- Weather category: \"Hot\", \"Cold\", \"Mild\" (exactly 3 options, nothing in between)\n",
    "- Email type: \"Spam\" or \"Not Spam\" (exactly 2 options)\n",
    "- Animal type: \"Cat\", \"Dog\", \"Bird\" (exactly 3 options)\n",
    "\n",
    "### Subtypes of Classification\n",
    "\n",
    "| Subtype | # of Categories | Example |\n",
    "|---------|-----------------|---------|\n",
    "| **Binary Classification** | Exactly 2 | Spam vs. Not Spam; Fraud vs. Legitimate |\n",
    "| **Multi-class Classification** | 3 or more | Cat vs. Dog vs. Bird; Setosa vs. Versicolor vs. Virginica |\n",
    "\n",
    "### Examples of Classification Problems\n",
    "\n",
    "| Problem | Input | Output (discrete) |\n",
    "|---------|-------|--------------------|\n",
    "| Spam detection | Email text | \"spam\" or \"not spam\" |\n",
    "| Disease diagnosis | Patient symptoms | \"positive\" or \"negative\" |\n",
    "| Handwriting recognition | Image of a digit | 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9 |\n",
    "| Sentiment analysis | Product review text | \"positive\", \"neutral\", or \"negative\" |\n",
    "| Weather type | Temperature, humidity | \"hot\" or \"cold\" |\n",
    "\n",
    "### Lecture Slide Example\n",
    "\n",
    "From the slide: \"Will it be hot or cold tomorrow?\" Answer: **Hot**\n",
    "\n",
    "This is classification because the answer is one of two categories (\"hot\" or \"cold\"), not a specific number. You cannot say \"it will be 73% hot\" -- it is either hot or cold.\n",
    "\n",
    "### Real-World Analogy: Sorting Mail into Boxes\n",
    "\n",
    "Classification is like a mail room worker sorting letters into labeled boxes: \"Bills\", \"Personal\", \"Junk\". Each letter goes into exactly ONE box. There is a fixed number of boxes (categories), and the worker must pick one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 5.3: Classification -- Predicting Hot or Cold\n",
    "# =============================================================================\n",
    "# Using the SAME weather data, but now we predict a CATEGORY\n",
    "# (\"hot\" or \"cold\") instead of a specific temperature number.\n",
    "# This perfectly matches the lecture slide example!\n",
    "# =============================================================================\n",
    "\n",
    "# --- Step 1: Convert continuous labels to discrete categories ---\n",
    "# We define: if tomorrow's temp >= 70°F, it's \"hot\" (1); otherwise \"cold\" (0).\n",
    "# This is how we turn a regression problem into a classification problem.\n",
    "\n",
    "THRESHOLD = 70  # degrees Fahrenheit\n",
    "\n",
    "# Create classification labels from the continuous temperature values.\n",
    "# np.where(condition, value_if_true, value_if_false)\n",
    "y_class = np.where(tomorrows_temp >= THRESHOLD, 1, 0)\n",
    "# 1 = \"hot\", 0 = \"cold\"\n",
    "\n",
    "class_names = ['Cold (< 70°F)', 'Hot (>= 70°F)']\n",
    "\n",
    "print(\"Classification Dataset (same data, different labels):\")\n",
    "print(f\"  Threshold: {THRESHOLD}°F\")\n",
    "print(f\"  'Hot' samples:  {sum(y_class == 1)}\")\n",
    "print(f\"  'Cold' samples: {sum(y_class == 0)}\")\n",
    "print()\n",
    "print(\"Compare REGRESSION vs CLASSIFICATION labels:\")\n",
    "print(f\"{'Tomorrow Temp (°F)':<22} {'Regression Label':<20} {'Classification Label'}\")\n",
    "print(\"=\" * 65)\n",
    "for i in range(8):\n",
    "    temp = tomorrows_temp[i]\n",
    "    reg_label = f\"{temp:.1f}°F\"\n",
    "    cls_label = \"HOT\" if y_class[i] == 1 else \"COLD\"\n",
    "    print(f\"  {temp:>10.1f}              {reg_label:<20} {cls_label}\")\n",
    "\n",
    "print()\n",
    "print(\"See the difference?\")\n",
    "print(\"  Regression:     the label is a specific number like 84.3°F\")\n",
    "print(\"  Classification: the label is a category like 'HOT' or 'COLD'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 5.3b: Train a classification model\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split data (same features, but using classification labels now).\n",
    "X_c_train, X_c_test, y_c_train, y_c_test = train_test_split(\n",
    "    X_weather, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train a logistic regression model.\n",
    "# Despite the name, Logistic Regression is a CLASSIFICATION algorithm!\n",
    "# It predicts probabilities and then assigns a category.\n",
    "clf_model = LogisticRegression(random_state=42)\n",
    "clf_model.fit(X_c_train, y_c_train)\n",
    "\n",
    "# Make predictions.\n",
    "y_c_pred = clf_model.predict(X_c_test)\n",
    "\n",
    "# Show predictions.\n",
    "print(\"Classification Predictions:\")\n",
    "print(f\"{'Actual':<12} {'Predicted':<12} {'Correct?'}\")\n",
    "print(\"=\" * 40)\n",
    "for i in range(min(15, len(y_c_test))):\n",
    "    actual = class_names[y_c_test[i]]\n",
    "    predicted = class_names[y_c_pred[i]]\n",
    "    correct = \"Yes\" if y_c_test[i] == y_c_pred[i] else \"No\"\n",
    "    print(f\"  {actual:<20} {predicted:<20} {correct}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "# Accuracy.\n",
    "clf_accuracy = accuracy_score(y_c_test, y_c_pred)\n",
    "print(f\"\\nClassification Accuracy: {clf_accuracy * 100:.1f}%\")\n",
    "print()\n",
    "print(\"The model outputs a CATEGORY ('Hot' or 'Cold'), NOT a number.\")\n",
    "print(\"This is what makes it CLASSIFICATION.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 5.3: Classification decision boundary\n",
    "# =============================================================================\n",
    "# This visualization shows how the classification model divides the space\n",
    "# into \"Hot\" and \"Cold\" regions.\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Create a mesh grid to visualize the decision boundary.\n",
    "# We create a grid of points covering the entire feature space,\n",
    "# then predict the class for each point to see where the boundary is.\n",
    "x_min, x_max = X_weather[:, 0].min() - 5, X_weather[:, 0].max() + 5\n",
    "y_min, y_max = X_weather[:, 1].min() - 5, X_weather[:, 1].max() + 5\n",
    "\n",
    "# np.meshgrid creates a grid of all (x, y) combinations.\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 200),  # 200 points along x-axis\n",
    "    np.linspace(y_min, y_max, 200)   # 200 points along y-axis\n",
    ")\n",
    "\n",
    "# Predict the class for every point in the grid.\n",
    "# np.c_ concatenates columns side by side.\n",
    "# .ravel() flattens a 2D array to 1D.\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = clf_model.predict(grid_points).reshape(xx.shape)\n",
    "\n",
    "# Draw the decision regions as colored backgrounds.\n",
    "# contourf fills the areas with colors.\n",
    "ax.contourf(xx, yy, Z, alpha=0.3, levels=[-0.5, 0.5, 1.5],\n",
    "            colors=['#3498db', '#e74c3c'])  # Blue for cold, red for hot\n",
    "\n",
    "# Plot the actual data points on top.\n",
    "cold_mask = y_class == 0\n",
    "hot_mask  = y_class == 1\n",
    "\n",
    "ax.scatter(X_weather[cold_mask, 0], X_weather[cold_mask, 1],\n",
    "           color='blue', s=60, edgecolors='black', alpha=0.7, label='Cold (actual)')\n",
    "ax.scatter(X_weather[hot_mask, 0], X_weather[hot_mask, 1],\n",
    "           color='red', s=60, edgecolors='black', alpha=0.7, label='Hot (actual)')\n",
    "\n",
    "ax.set_xlabel(\"Today's Temperature (°F)\", fontsize=13)\n",
    "ax.set_ylabel('Humidity (%)', fontsize=13)\n",
    "ax.set_title('CLASSIFICATION: Hot vs Cold Decision Boundary', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The colored background shows the model's decision regions:\")\n",
    "print(\"  Blue region: model predicts 'COLD'\")\n",
    "print(\"  Red region:  model predicts 'HOT'\")\n",
    "print(\"  The boundary between them is the 'decision boundary'.\")\n",
    "print(\"  New data points are classified based on which region they fall in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.4 Regression vs Classification: Side-by-Side Comparison\n",
    "\n",
    "This is one of the most important distinctions in supervised learning. Let us make it crystal clear.\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|--------|-----------|----------------|\n",
    "| **Output type** | Continuous number | Discrete category |\n",
    "| **Example output** | 84.3°F | \"Hot\" |\n",
    "| **Possible outputs** | Infinite (any real number) | Finite (from a fixed set) |\n",
    "| **Question format** | \"How much?\" / \"How many?\" | \"Which one?\" / \"What type?\" |\n",
    "| **Slide example** | Temperature = 84°F | Hot or Cold? = Hot |\n",
    "| **Error measurement** | Distance (how far off?) | Correct or incorrect |\n",
    "| **Common algorithms** | Linear Regression, Ridge, Lasso | Logistic Regression, SVM, Decision Trees |\n",
    "| **Visualization** | Best-fit line through points | Decision boundary separating regions |\n",
    "\n",
    "### The Same Data, Two Different Questions\n",
    "\n",
    "Notice that we used the **exact same weather data** for both regression and classification! The difference is in the **question we ask**:\n",
    "\n",
    "- **Regression question**: \"What will the exact temperature be tomorrow?\" --> 84°F\n",
    "- **Classification question**: \"Will it be hot or cold tomorrow?\" --> Hot\n",
    "\n",
    "The type of question determines whether you use regression or classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION 5.4: Side-by-side comparison of Regression vs Classification\n",
    "# =============================================================================\n",
    "# This is the KEY visual that ties Slide 5 together.\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ---- LEFT: REGRESSION ----\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Plot the data.\n",
    "ax1.scatter(todays_temp, tomorrows_temp, color='steelblue', alpha=0.5,\n",
    "            s=40, edgecolors='navy')\n",
    "\n",
    "# Plot the regression line.\n",
    "x_plot = np.linspace(25, 105, 100).reshape(-1, 1)\n",
    "x_plot_full = np.column_stack([x_plot, np.full(100, humidity.mean())])\n",
    "y_plot = reg_model.predict(x_plot_full)\n",
    "ax1.plot(x_plot, y_plot, color='red', linewidth=2.5, label='Regression line')\n",
    "\n",
    "# Highlight a prediction.\n",
    "ax1.scatter([80], [reg_model.predict([[80, 55]])], color='gold', s=300,\n",
    "            marker='*', zorder=10, edgecolors='black', linewidth=2)\n",
    "ax1.annotate(f'Prediction: {reg_model.predict([[80, 55]])[0]:.1f}°F',\n",
    "             xy=(80, reg_model.predict([[80, 55]])[0]),\n",
    "             xytext=(45, reg_model.predict([[80, 55]])[0] + 12),\n",
    "             fontsize=12, fontweight='bold', color='darkred',\n",
    "             arrowprops=dict(arrowstyle='->', color='darkred', lw=2))\n",
    "\n",
    "ax1.set_xlabel(\"Today's Temperature (°F)\", fontsize=13)\n",
    "ax1.set_ylabel(\"Tomorrow's Temperature (°F)\", fontsize=13)\n",
    "ax1.set_title('REGRESSION\\n\"What will the temperature be?\"\\nAnswer: A specific NUMBER (e.g., 84°F)',\n",
    "              fontsize=13, fontweight='bold', color='darkblue')\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# ---- RIGHT: CLASSIFICATION ----\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Plot hot and cold points.\n",
    "cold_m = y_class == 0\n",
    "hot_m = y_class == 1\n",
    "ax2.scatter(todays_temp[cold_m], humidity[cold_m],\n",
    "            color='blue', s=60, edgecolors='black', alpha=0.7, label='Cold')\n",
    "ax2.scatter(todays_temp[hot_m], humidity[hot_m],\n",
    "            color='red', s=60, edgecolors='black', alpha=0.7, label='Hot')\n",
    "\n",
    "# Highlight a prediction.\n",
    "new_point_class = clf_model.predict([[80, 55]])[0]\n",
    "label_text = \"HOT\" if new_point_class == 1 else \"COLD\"\n",
    "ax2.scatter([80], [55], color='gold', s=300, marker='*', zorder=10,\n",
    "            edgecolors='black', linewidth=2)\n",
    "ax2.annotate(f'Prediction: {label_text}',\n",
    "             xy=(80, 55), xytext=(45, 75),\n",
    "             fontsize=12, fontweight='bold', color='darkred',\n",
    "             arrowprops=dict(arrowstyle='->', color='darkred', lw=2))\n",
    "\n",
    "ax2.set_xlabel(\"Today's Temperature (°F)\", fontsize=13)\n",
    "ax2.set_ylabel('Humidity (%)', fontsize=13)\n",
    "ax2.set_title('CLASSIFICATION\\n\"Will it be hot or cold?\"\\nAnswer: A CATEGORY (Hot or Cold)',\n",
    "              fontsize=13, fontweight='bold', color='darkred')\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LEFT (Regression):     Output is a specific NUMBER  (e.g., 84.0°F)\")\n",
    "print(\"RIGHT (Classification): Output is a CATEGORY/LABEL  (e.g., 'Hot')\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nSame data. Same features. DIFFERENT type of prediction.\")\n",
    "print(\"The type of question you ask determines which approach to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INTERACTIVE EXERCISE: Test Your Understanding\n",
    "# =============================================================================\n",
    "# For each problem below, decide if it is REGRESSION or CLASSIFICATION.\n",
    "# Then run the cell to see the answers!\n",
    "# =============================================================================\n",
    "\n",
    "problems = [\n",
    "    (\"Predict the price of a used car\", \"Regression\",\n",
    "     \"Price is a continuous number ($15,230.50, $22,000, etc.)\"),\n",
    "    (\"Predict if a tumor is benign or malignant\", \"Classification\",\n",
    "     \"Only 2 categories: benign or malignant\"),\n",
    "    (\"Predict how many inches of rain will fall\", \"Regression\",\n",
    "     \"Rainfall is a continuous number (2.5 inches, 0.3 inches, etc.)\"),\n",
    "    (\"Predict which genre a movie belongs to\", \"Classification\",\n",
    "     \"Fixed categories: action, comedy, drama, horror, etc.\"),\n",
    "    (\"Predict a student's final exam score\", \"Regression\",\n",
    "     \"Score is a continuous number (92.5, 78.3, etc.)\"),\n",
    "    (\"Predict if a customer will buy a product\", \"Classification\",\n",
    "     \"Only 2 categories: buy or not buy\"),\n",
    "    (\"Predict the number of visitors to a website tomorrow\", \"Regression\",\n",
    "     \"Visitor count is a number (though whole numbers, treated as continuous)\"),\n",
    "    (\"Predict which digit (0-9) is in a handwritten image\", \"Classification\",\n",
    "     \"Exactly 10 fixed categories: 0, 1, 2, ..., 9\"),\n",
    "]\n",
    "\n",
    "print(\"QUIZ: Regression or Classification?\")\n",
    "print(\"=\" * 75)\n",
    "print()\n",
    "\n",
    "for i, (problem, answer, explanation) in enumerate(problems, 1):\n",
    "    print(f\"  {i}. {problem}\")\n",
    "    print(f\"     --> ANSWER: {answer}\")\n",
    "    print(f\"     --> WHY:    {explanation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary of Slide 5: Types of Supervised Learning\n",
    "\n",
    "| Type | Predicts | Example from Lecture | Key Question |\n",
    "|------|----------|---------------------|-------------|\n",
    "| **Regression** | Continuous values (numbers) | Temperature = 84°F | \"How much?\" |\n",
    "| **Classification** | Discrete categories (labels) | Hot or Cold = Hot | \"Which one?\" |\n",
    "\n",
    "**Remember**: The type of output (number vs. category) determines whether it is regression or classification. The same input data can be used for either -- it depends on the question you ask!\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Summary: Lecture 2, Slides 2-5\n",
    "\n",
    "---\n",
    "\n",
    "## Everything We Learned, All in One Place\n",
    "\n",
    "### Slide 2: What is Supervised Learning?\n",
    "\n",
    "- **Supervised learning** = learning from **labeled data** (data with known correct answers)\n",
    "- Data is organized as pairs: $(x_i, y_i)$ where $x_i$ is the input and $y_i$ is the label\n",
    "- The **goal** is to predict the correct label $y$ for a brand-new, unseen input $x$\n",
    "- **Deep learning** uses neural networks (input layer --> hidden layers --> output layer) to make predictions\n",
    "\n",
    "### Slide 3: Examples of Supervised Learning\n",
    "\n",
    "Supervised learning powers many real-world applications:\n",
    "- Image Classification, Document Categorization, Speech Recognition\n",
    "- Protein Classification, Spam Detection, Branch Prediction\n",
    "- Fraud Detection, NLP, Playing Games, Computational Advertising\n",
    "- In every case: learn from labeled examples, then predict on new data\n",
    "\n",
    "### Slide 4: Supervised Learning Workflow\n",
    "\n",
    "Every supervised learning project follows the same pipeline:\n",
    "1. **Labeled Data + Labels** --> 2. **Learning Algorithm** --> 3. **Trained ML Model** --> 4. **Test Data** --> 5. **Predictions**\n",
    "- In code: `model.fit(X_train, y_train)` then `model.predict(X_test)`\n",
    "\n",
    "### Slide 5: Types of Supervised Learning\n",
    "\n",
    "| Type | Output | Slide Example |\n",
    "|------|--------|---------------|\n",
    "| **Regression** | Continuous number | Temperature = 84°F |\n",
    "| **Classification** | Discrete category | Hot or Cold = Hot |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Vocabulary\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| Supervised Learning | ML where the model learns from labeled data |\n",
    "| Labeled Data | Data samples where the correct answer (label) is known |\n",
    "| Input ($x$) / Features | The information fed into the model |\n",
    "| Output ($y$) / Label / Target | The correct answer the model tries to predict |\n",
    "| Training | The process of the model learning from labeled data |\n",
    "| Prediction / Inference | Using the trained model on new, unseen data |\n",
    "| Regression | Predicting a continuous numerical value |\n",
    "| Classification | Predicting a discrete category from a fixed set |\n",
    "| Neural Network | A model with input, hidden, and output layers |\n",
    "| Deep Learning | ML using neural networks with many layers |\n",
    "| $\\hat{y}$ (y-hat) | The model's predicted value (estimate of the true $y$) |\n",
    "\n",
    "---\n",
    "\n",
    "## What is Next?\n",
    "\n",
    "Now that you understand the fundamentals of supervised learning, the next lectures will dive deeper into:\n",
    "- Specific algorithms (how does a neural network actually learn?)\n",
    "- Loss functions (how do we measure prediction errors?)\n",
    "- Training techniques (gradient descent, backpropagation)\n",
    "- And eventually: how Large Language Models (LLMs) use these foundations!\n",
    "\n",
    "**Congratulations on completing your first steps into machine learning!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}